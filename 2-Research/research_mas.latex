\section{Multi-Agent Systems}
\label{sec:mas}
% Simulation uncertainty
% TODO Why are we using it?
% TODO Show some alternative simulation models to multi agent systems
% ASSIGNED TO: Adam, Ricky, Bo

\subsection{What is a Multi-Agent System?} % (fold)
A multi-agent system (MAS) is program which comprises of a series of interacting 
intelligent agents.
These systems are used to solve problems that would be problematic, or often impossible,
for a monolithic approach to resolve.
The three key principles of such a framework are autonomy, locality and decentralisation. 
Each agent must be independently autonomous within the specified environment.
No agent should be able to have a full view or understanding of the system they 
inhabit.
No agent should be assigned a controlling role, in which they are instantiated to dictate 
the behaviour of other agents. 
By monitoring how the agents interact with their environment and each other, results 
can be drawn to gain a fuller grasp of the initial problem.
% What is a Multi-Agent System (end)
\subsection{Formal Definition of the Multi-Agent System} % (fold)
\label{sub:mas_fd}

	A MAS can be defined as a 3-tuple\cite{mas_book,fagin_95}
	\begin{equation}
		W = <E,e_0,\tau>
		\label{eq:mas}
	\end{equation}
	where $E$ is the set of all possible environment states the system can be in, $e_0$ is the initial state the environment is in and $\tau$ is the state transformer function which takes the current state of the environment as input and outputs the next state. 

%TODO: Correct over-use of the word "state"
	Note that the state transformer function $\tau$ does not necessarily give the next state based on the current state alone, due to the fact that $\tau$ is likely to be also dependent on previous transitions.
	It is convenient to introduce a series of states that represents the history of transitions from the initial state $e_0$ to the current state, that is a run of the system.
	A run $r$ is represented as follows---
	\begin{equation}
		r: e_0 \xrightarrow{\alpha_0} e_1 \xrightarrow{\alpha_1} \ldots \xrightarrow{\alpha_n} e_{n+1} \xrightarrow{\alpha_{n+1}} \ldots
		\label{eq:run}
	\end{equation}
	where $e_i$ and $\alpha_i$ respectively denote the environment states and the actions performed by the agents.

	Let $R$ be the set of all possible runs, the state transformer function $\tau$ can be seen as\footnote{This formulation does not imply agents have a complete picture of the environment. It is highly unlikely, and in many cases impossible, for agents to have a complete understanding.}---
	\begin{equation}
		\tau : R \rightarrow \wp (E).
		\label{eq:state_transformer}
	\end{equation}

	It is also noteworthy that $\tau$ gives an element of $\wp (E)$, i.e. a subset $E'$ of $E$, as its output.
	Each element of $E'$ are associated with a probability to be chosen and an environment state $e' \in E'$ would be chosen as the environment state of the next iteration.
	This allows non-deterministic evolution of the system. 

	Finally the definition of $R$, which bears a resemblance to the same idea of the set of paths in a graph traversal problem, can be reciprocally given using $\tau$ by induction.
	\begin{align}
		& R			= \cup_{i=1}^\infty R_i, \nonumber \\
		& R_0		= \{ < e_0 > \}, \nonumber \\
		& R_{i+1}	= \{ r_i ++ < e_{i+1} > |
			e_{i+1} \in \tau(r_i) \wedge
			r_i \in R_i
		\}.
		\label{eq:set_of_runs}
	\end{align}

	However it is important to recognise that although a run of a system is similar to a path in a graph in many ways, it is in most cases impractical to traverse the multi-agent system with any graph traversal algorithms.
	% TODO proofread
	For one thing doing this may not give us any insightful information of the system.
	Owing to the fact the environment should not necessarily reach a goal state, or simply the goal state cannot be defined by the semantics within the multi-agent system.
	For another, the number of all environment states could be non finite, an exhaustive search without a goal state is most likely impractical.

% subsection Formal Definition of the Multi-Agent System (end)
\subsection{Agents} % (fold)
\label{sub:agents}
% TODO need citations

\subsubsection{Purely Reactive Agent} % (fold)
\label{subs:purely_reactive_agent}

	Purely reactive agents take actions that depend only on the current conditions of the environment, disregarding the past completely.

	\begin{equation}
		Action : E \rightarrow Ac
		\label{eq:action_def}
	\end{equation}

	For example: a simple thermostat is a purely reactive agent. 
	It's job is to keep the temperature constant inside a given area.
	There are two states to this system: one where the room temperature is below a threshold value, which we designate state $e_1$; and a second where the temperature above the threshold, which we regard as state $e_2$.
	For this particular agent, if it is in environment $e_1$, it just simply takes performs the action that turns the heater on.
	Otherwise, in state $e_2$, it will the heater off.
	In this system, the agent is purely reactive, only making decisions based on its current environment state and nothing else.

% subsubsection Purely Reactive Agent (end)
\subsubsection{Perceptive Agent} % (fold)
\label{subs:percept_agent}

	A perceptive agent is slightly more complex than the purely reactive agent.
	It now takes two steps to make decisions. 
	The agent has the ability to observe the environment with a `see function' prior to making 
a decision on what action it will take.
	\begin{figure}[ht]
		\centering
		\includegraphics[width=0.4\linewidth]{figures/perceptive_agent}
		\caption{Perceptive agent}
		\label{fig:perceptive_agent}
	\end{figure}

	% Graph needed here
	This `see function' maps the environment state E to a percept of the agent's view of the world. 
	\begin{equation}
		See : E \rightarrow Per
		\label{eq:per_see}
	\end{equation}
	The action stage now maps a sequence of perceptions to actions instead of mapping environment to actions directly.
	\begin{equation}
		Action : Per^* \rightarrow Ac
		\label{eq:per_action}
	\end{equation}
	$Per^*$ is a sequence of perceptions because the agent can record all it has seen in the past, indicating that it can learn from history and then make decisions. This paradigm can be further improved with the concept of agents with states.

% subsubsection Perceptive Agent (end)
\subsubsection{Agent with States} % (fold)
\label{subs:agent_with_states}

	\begin{figure}[ht]
		\centering
		\includegraphics[width=0.4\linewidth]{figures/agent_with_state}
		\caption{agent with state}
		\label{fig:agent_with_state}
	\end{figure}
	% Graph needed here
	These agents can now maintain states that are internal data structures of agents and can be used to record information about the environment state and the history. 
	So we introduce I to be the set of all internal states.
	For this type of agent, the see function doesn't change at all, still maps the environment state E to the percept Per.
	\begin{equation}
		See : E \rightarrow Per
		\label{eq:states_see}
	\end{equation}

	But for the action part, it is now defined in a different way as the interference of those internal states of agents.
	The action of a agent is actually decided by the set of internal states $I$:
	\begin{equation}
		Action : I \rightarrow Ac
		\label{eq:states_action}
	\end{equation}

	And how we can use the percept to make some progress on the internal states? Here we introduce a function $next$ which works like feedback.
	This function maps an internal state and a percept to a new internal state.
	It seems like the agent is learning something slowly form the changing environment states and each time varies its internal states as well.
	\begin{equation}
		Next : I \times Per \rightarrow I
		\label{next}
	\end{equation}

	So in conclusion, the agent first observes the environment, then it modifies the internal states according to the concept and the current internal states.
	Depends on this new achieving internal state, the agent can carry on with different actions.
	This action will somehow affect the environment and update the environment states. 
	This goes on as a loop forever until the agent dies.

% subsubsection Agent with States (end)
\subsubsection{BDI Agents} % (fold)
\label{BDI_agents}

	BDI agent is a much more complicated agent with an internal architecture which contains 5 basic attributes:
	\begin{itemize}
			\item Beliefs : decide the properties of agents and can be updated due to perceptions.
			\item Desires : define what agent wants to do.
			\item Events : any action that an agent can take on.
			\item Intentions : which are much stronger than desire.
			\item Plan : it is an event or a goal which can be replaced by an appropriate plan at the time they need to be achieved.
	\end{itemize}
	\begin{figure}[ht]
		\centering
		\includegraphics[width=0.4\linewidth]{figures/BDI_agent}
		\caption{BDI agent}
		\label{fig:bdi_agent}
	\end{figure}
	% Graph needed here.

	The way BDI agent acts is not much different from other types of agents.
	The agent first perceives the environment, updates its beliefs and check which event that it can perform.
	It also requires to check with its desires and intentions in the store to decide which event to choose or even any goals may be achieved.
	Of course plans are also contributing to make decisions about how to matching the goal or subgoal.
	Then it selects an event and make a move, indeed this event needs to be deleted from the event store afterwards and agent attributes update again.

% subsubsection BDI Agents (end)

% subsection Agents (end)
\subsection{Genetic Algorithm in MAS} % (fold)
\label{sub:genetic_algorithms}
% TODO need citations
% TODO proper graphs

	Finding an optimal solution of a problem sometimes is impossible but we can somehow improve an existing solution.
	This notion introduces us to genetic algorithms which pretty much approach this goal in a similar manner. 
	In an multi-agent system, using selection, crossover and mutation of agents, and evaluating their success according to their fitness function, we can improve an existing solution. 
	Consequently we plan to focus on the relation between genetic algorithm and MAS in order to define some new properties of agents which can be applied to those mentioned functions.

\subsubsection{Representation of Agents} % (fold)
\label{subs:representation_of_agents}

	Similar to evolutionary algorithms simulating a Darwinian process, MAS can simulate the evolution of agents. 	But before that, we need to define each agent with some characterized chain of bits whose length will be related to the data structure of different agents.
	Various sequences of the chain stand for corresponding properties of agents.
	And by applying those four functions to the binary representation of agent, we can keep evolving our solution and finally meet some optimal answers. 
	Next, let's look in further details of those functions corresponding to the genetic algorithm.

% subsubsection Representation of Agents (end)
\subsubsection{Mutation} % (fold)
\label{subs:mutation}

	Suppose now we have a set of agents and each agent has a binary sequence representation of its behaviour. 
	Mutation is simply converting one bit of the binary chain in order to gain a new characteristic of the agent that is more adaptive to the environment.
	\begin{equation}
		Converting~bit: 1\rightarrow 0,~0\rightarrow 1
	\end{equation}
	In the case of hunting game, we can make our agent be more intelligent by changing some relevant parameters.
	For instance, the agent may probably hunt at a higher efficient or even be able to form stronger groups so that can hunt we high probability of success. 
	But mutate too much may introduce a certain aggressiveness at the agent level, and consequently the agent will perhaps do something bad and loose its social cohesion.
	Therefore in order to avoid this situation, the mutation interventions by genetic algorithms will need to be weak.
	Nevertheless, we can tolerant that the mutation can be rapid at the beginning as we need the agents to grow fast. 
	And moreover, there is no leadership at the first place since the groups are forming randomly.
	After some period, the mutation should be slow in order to avoid breaking the process of evolution by dramatically modifying the agent’s characteristic.

% subsubsection Mutation (end)
\subsubsection{Crossover} % (fold)
\label{subs:crossover}

	Unlike the mutation process, the crossover process uses the "genome" of two parent agents to produce a third genome. 
	Of course this crossover process is random, so it may produce something useless, but we can maintain the beneficial parts and get rid of the disadvantageous traits by using a selection function which we will talk about later.
	\begin{figure}[ht]
		\centering
		\includegraphics[width=0.4\linewidth]{figures/crossover}
		\caption{crossover}
		\label{fig:crossover}
	\end{figure}
	Now, let's consider an agent as a pie chart instead of binary chain just for better explanation.
	We can randomly pick a break point in the chain, as shown in the diagram, we can cut the pie any way we prefer.
	After that, one combines those two different parts from separate agents so that a new agent with unique character is formed. 
	Though the use of a simple crossing does not always give us good results, a multiple crossing may allow us to end up in a more. 

% subsubsection Crossover (end)
\subsubsection{Fitness Function} % (fold)
\label{subs:fitness_function}

	The fitness function is to evaluate the degree of utility of each agent, as well as the fitness of the agent to the environment.
	This function is actually quite flexible and depends on how successful we judge the agents are in performing actions and making decisions.
	For example, we can test the efficiency of hunting for each agent where the efficiency is hunting results divided by the time used to hunt. 
	Those agents with higher efficiency will be chosen as the parents of the next generation and the rest will die out time after time. 
	Furthermore, we can also evaluate the reputation of each group leader under the hunting performance of the group. 
	Obviously, those leaders with good reputation will survive.
	In conclusion, the fitness function acts as a tool to decide which part of agents is fit to the evolution process.

% subsubsection Fitness Function (end)
\subsubsection{Selection} % (fold)
\label{subs:selection}

	Bear in mind that the processes of mutation and crossover are both randomly performed which means offspring are inherited from ancestors in every possible way. 
	This leads to the diversity of new born agents.
	Some of them may perform better and some of them may be worse.
	Nevertheless, our purpose of applying genetic algorithm is to find a better solution, so here we introduce selection function to achieve this target. 
	This selection function is used after the fitness function because we need to first evaluate which part of agents is better.
	Consequently, a utility parameter of integer type from 0 to 10 can be assigned to each agent with increasingly performance.
	The parameter may be computed depends on the hunting ability of the agent as well as the communication ability which can be evaluated by the reputation value. 
	After we have this utility value for each agent, a threshold can be set and we just select those agents with higher utility value and the rest is abandoned.

	This is basically how genetic algorithm works, it allows the agent to mutate and evolve by itself in order to get a new agent with better performance.  

% subsubsection Selection (end)

% subsection Genetic Algorithm in MAS (end)
\subsection{Neural Networks} % (fold)
\label{sub:neural_networks}
% TODO proofread

	A neuron of a neural network is a node that evaluates a weighted sum of all its inputs to give an output with an activation function $f$.
	The weights of each input is given as constant values, these values can be tweaked to allow better output values from the neuron.
	Hence the output $y$ is given as follows---
	\begin{equation}
		y = f \left( \sum_i{w_i x_i}, b \right),
		\label{eq:neuron}
	\end{equation}
	where $x_i$ is the $i$th input value, $w_i$ is the weight constant for the $i$th input, and $b$ is the bias constant.
	It is noteworthy that while the activation function could be any function, it is generally a sigmoid function which takes the weighted sum and a bias constant to give a result bounded in range $[-1,1]$.
	\begin{equation}
		f: \mathbf{R \times R} \rightarrow \mathbf{R}
		\label{eq:activation}
	\end{equation}

	A feed forward neural network (FFNN) is a network of neurons.
	It consists of layers of neurons, where in each layer neurons do not interact.
	Only outputs from the previous layer are fed to the consecutive layer, this ensures that the network has no feedback loops.
	To enable learning behaviour of the multi-agent system and agents within it, feed forward neural networks are employed.
	It is used to give actions of an agent based on the agent's internal states.

	Training a neural network with conventional training algorithms such as back-propagation algorithm is very likely to be an impossible task.
	These methods would often require a training set, that is a set of truths consists of pairs of inputs and outputs to eventually allow correct evaluation of these results with the neural network through training.
	However in a multi-agent simulation, it is often clearly implausible to obtain such a training set.
	Using genetic algorithm to train neural networks solves this problem, by having a fitness function to indirectly guide the evolution of neural networks towards giving strategies that have better outcomes.

% subsection Neural Networks (end)
\subsection{Validation of the Agent-Based Simulation} % (fold)
\label{sub:validation}

\subsubsection{Necessity of Validation} % (fold)
\label{subs:validation_necessity}

	% detailed reasons besides structural problems, especially time \& space quantisation
	Recall the definition of a run of a multi-agent system above in \eqref{eq:run}, it is immediately clear that the system evolves in discrete time.
	While this may not appear as an immediate problem with simulation, as finer the time interval between iterations generally implies smaller errors.
	However as agents start making decisions based on a history of the environment states, the quantisation errors at previous iterations begin to accumulate.
	Consider the following scenario: the environment provides a function $f(t)$, which is dependent on time, and an agent in the environment calculates the following value at iteration $n$---
	\begin{equation}
		y_{n} = y_{n-1} + f(t_{n-1})\Delta t_{n-1},
		\quad \text{with~} y_{0} = 0.
	\end{equation}
	Or equivalently,
	\begin{equation}
		y_{n} = \sum_t f(t)\Delta t \approx \int_t f(t)dt.
	\end{equation}
	Hence this is an example of using Euler's method to integrate $f(t)$ over time.
	However it is clear that Euler's method gives imprecise results, which is best illustrated with Figure~\ref{fig:euler}.
	\begin{figure}[ht]
		\centering
			\includegraphics[width=0.4\linewidth]{figures/euler}
		\caption{The imprecise nature of Euler's method}
		\small{The smooth line represents the continuous time integration of $f(t)$.
			The line joining dots is the discrete time variation using Euler's method.}
		\label{fig:euler}
	\end{figure}

	% TODO need justification, will be looked at after simulation results.
	In general, the problem with quantisation of time worsens if one must work with agents could give abrupt changes in decisions under certain cases.
	Because this would possibly make outputs in steady state converge to imprecise values, as well as causing phase shifts in oscillatory measurements. 

% subsubsection Necessity of Validation (end)
\subsubsection{Validation With Sensitivity Analysis} % (fold)
\label{subs:validation_sensitivity_analysis}

	One of the methods of validation that can be employed is sensitivity analysis.
	Sensitivity analysis can often give indications of design problems that did not anticipate at the design process.
	It is performed by varying combinations of input parameters of the multi-agent system, and comparing the output results after a number of iterations.
	The sensitivity matrix for the system given input vector $\mathbf{X}_0$ with output vector $\mathbf{Y}$ which is dependent on the input vector, is defined below.
	This matrix can be understood as the change of every output measurements with respect to the changes of every input parameters.
	\begin{align}
		S(\mathbf{X}_0)
		&	=
			\left[
				\nabla_{\mathbf{X}} \mathbf{Y}
			\right]_{\mathbf{X} = \mathbf{X}_0} \nonumber \\
		&	=
			\left[
				\begin{pmatrix}
					\frac{\partial}{\partial X_0} &
					\frac{\partial}{\partial X_1} &
					\ldots &
					\frac{\partial}{\partial X_M}
				\end{pmatrix}^\top \mathbf{Y}^\top
			\right]_{\mathbf{X} = \mathbf{X}_0} \nonumber \\
		&	=
			\begin{bmatrix}
				\frac{\partial Y_0}{\partial X_0} &
				\frac{\partial Y_0}{\partial X_1} &
				\ldots &
				\frac{\partial Y_0}{\partial X_M} \\
				\frac{\partial Y_1}{\partial X_0} &
				\frac{\partial Y_1}{\partial X_1} &
				\ldots &
				\frac{\partial Y_1}{\partial X_M} \\
				\vdots & \vdots & \ddots & \vdots \\
				\frac{\partial Y_N}{\partial X_0} &
				\frac{\partial Y_N}{\partial X_1} &
				\ldots &
				\frac{\partial Y_N}{\partial X_M}
			\end{bmatrix}_{\mathbf{X} = \mathbf{X}_0}
	\end{align}
	where
	\begin{equation*}
		\mathbf{X} =
			\begin{pmatrix}
				X_0 & X_1 & \cdots & X_M
			\end{pmatrix}^\top, \quad
		\mathbf{Y} =
			\begin{pmatrix}
				Y_0 & Y_1 & \cdots & Y_N
			\end{pmatrix}^\top,
	\end{equation*}
	both denote the input and output vectors respectively.
	To look for anomalies in the system, one can observe abrupt changes in the euclidean norm of $S$, $||S(\mathbf{X})||$ as one varies the value of $\mathbf{X}$, or abrupt changes in each of the elements of the sensitivity matrix.\footnote{Smaller input/output vectors could make the observation much more manageable.}
	% should this part below belong here?
	Besides validation of the system, the sensitivity matrix can be used as a way to prune insignificant input parameters and output measurements.\cite{mas_validation}
	An input parameter $X_i$ may be pruned if it satisfies the following condition
	\begin{equation}
		\forall \mathbf{X}
		\left(
			\max_{0 \leq j \leq N}
			\frac{\partial Y_j}{\partial X_i}(\mathbf{X})
			< \epsilon
		\right) ,
	\end{equation}
	where $\epsilon$ is some small threshold value, for the reason that all output measurements are changed with insignificant magnitudes in their values, it is thus safe to conclude that the particular input parameter does not impact the system.
	Similarly an output measurement $Y_j$ is of little consequence if it satisfies
	\begin{equation}
		\forall \mathbf{X}
		\left(
			\max_{0 \leq i \leq M}
			\frac{\partial Y_j}{\partial X_i}(\mathbf{X})
			< \epsilon
		\right) ,
	\end{equation}
	since it does not change significant enough for all variances of inputs, it does not provide much insight to the system by changing input parameters.

% subsubsection Sensitivity Analysis (end)
\subsubsection{Comparing Results With Expected Outcomes By Regression Analysis} % (fold)
\label{subs:validation_comparision}

	Besides validation with sensitivity analysis, it is also possible to test the simulation outcomes against some of the well established theories.
	By way of illustration, in a simulated ecosystem one could use the generalised version of the Lotka-Volterra equations\cite{mas_lotka1,mas_lotka2} to perform regression analysis of the populations of all species within the system.
	The generalised Lotka-Volterra equations in a system with $N$ different species are given by
	\begin{equation}
		\frac{dx_i}{dt} =
			\left(
				r_i - \sum_j {a_{ij} x_j}
			\right)
			x_i,
		\label{eq:general_lv}
	\end{equation}
	where $i \in [0,1,\ldots,N-1]$, with $x_i$ of vector $\mathbf{x}$ representing the population of the $i$th specie.
	The value of $r_i$ of the vector $\mathbf{r}$ is the intrinsic growth rate of the $i$th specie.
	It denotes how fast the population of a species grow or decline without interacting with any other species in the system.
	Generally a species with $r \leq 0$ means it must depend on predating other species to survive, while a species with $r \geq 0$ means its population grow could grow indefinitely and must be the target for predation or competition, or its population is unbounded.
	Therefore there exists some $j$th species that either predates it, equivalently, $a_{ij} > 0$ and $a_{ji} < 0$ to control the growth of the $i$th species and the $j$th species must benefit from predation, or competes with it for resources, equivalently, $a_{ij} > 0$ and $a_{ji} > 0$, which means none of them benefits from the growth of each other.\footnote{The could be further discussed when we deal with the ecosystem we defined for the simulation, as stags and hares are assumed to grow intrinsically, and hunters must benefit from hunting them.}
	The matrix $A$ is hence introduced to represent the interaction between species.
	\begin{equation}
		A =
		\begin{pmatrix}
			a_{00} & a_{01} & \ldots \\
			a_{10} & a_{11} & \ldots \\
			\vdots & \vdots & \ddots
		\end{pmatrix}
	\end{equation}

	By finding the change of population of the $i$th species $\Delta x_i$, and the time spent for the current iteration $\Delta t$, it immediately follows that
	\begin{equation}
		\frac{dx_i}{dt} \approx \frac{\Delta x_i}{\Delta t}.
		\label{eq:dxdt_approx}
	\end{equation}

	With equations \eqref{eq:general_lv} and \eqref{eq:dxdt_approx}, one can easily perform linear regression in the $(\mathbf{x},\frac{d\mathbf{x}}{dt})$ space to find the values of the intrinsic growth rate vector $\mathbf{r}$ and the interaction matrix $A$.

% subsubsection Comparing Results With Expected Outcomes (end)

% subsection Validation of the Agent-Based Simulation (end)

% section Multi-Agent Systems

% vim: set fdm=marker fmr=(fold),(end) nolist:
